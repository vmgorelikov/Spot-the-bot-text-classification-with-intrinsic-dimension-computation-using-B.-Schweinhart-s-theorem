Spot the bot: text classification with intrinsic dimension computation using B. Schweinhart's theorem

This repository contains the code and some of the data for my term paper dedicated to creating a classifier that could distinguish texts written by humans and generated by LLMs.

# How to use
1. Download the files from this repository.
   
2. Make sure you have all of the required modules installed.

```pip install numpy spacy sklearn tqdm```

```python -m spacy.ru.download ru_core_news_sm```

3. MST-Clustering should be installed with

```pip install git+https://github.com/whiteroomlz/mst-clustering.git```

4. See code in question if something goes wrong.
   
5. Run ```python classifier.py <the text file with the text you would like to classify> [the model to be used] [the dictionary to be used]```. The text file must be properly formatted, with no newline characters occuring within the sentences in it. The model must be a joblib dump of a trained classifier. The dictionary must be a NumPy pickle dump of a ```dict``` containing ```numpy.ndarray``` vector representations for each word. The program will print a line with the result.

# Files by sections of the paper
## Sections 4, 5

```bambara_processor.py``` — the processor for Bambara texts. Usage: ```python bambara_processor.py <a directory containing raw texts> <an output file for texts in lines> [a directory for output texts in files, sentences in lines]```

## Sections 4, 6

```corpora/``` — the corpora used in the study, truncated to 100 MB due to the file size limit on GitHub. Should be used to check the models' accuracy.

```dictionaries/Russian_dict_SVD_100.npy``` — the dictionary used in the study, cropped to the actually used dimensions.

```text_to_vectors.py``` — the script for converting tokens into a time series. Only used as an imported module. The sole function accepts a list of tokens, a dictionary of format described in pt. 5 of the How to use section, n-gram length, embeddings dimension, one of two strings describing the desired output ("sequence" for time series and "unique" for an array of unique n-grams) and a set in which words that are not in the dictionary will be added. The function returns the desired output and the set with unprocessed words.

```vectors_to_schweinhart.py``` — the script for estimating intrinsic dimensions of a time series. Only used as an imported module. The two of its functions excplicitly used are ```build_mst``` and ```compute_dimensions```. ```build_mst``` accepts an array of vectors and returns a minimum spanning tree. ```compute_dimensions``` accepts the same array of vectorsm a minimum spanning tree and a single value of alpha or a set of parameters for numpy.linspace to generate a sequence of alphas. It returns an array with estimated dimensions for each value of alpha.

```texts_to_schweinhart.py``` — the script which was used to compute the intrinsic dimensions for all texts. It takes a corpus of preprocessed texts written by humans, a corpus of generated texts and a dictionary of the format described above. It outputs batches to the ```batches``` directory as described below.

```batches/``` — the batches with sets of ten computed intrinsic dimensions computed for each text with names of format ```<max alpha>_<number of alphas>_<batch number>_X.pkl.npy``` and the corresponding text classes with names of format  ```<max alpha>_<number of alphas>_<batch number>_y.pkl.npy```

## Section 7
```train.py``` — the script which was used for training and evaluating the 42 models. It accepts batches from the ```batches/``` directory and writes the joblib dumps of the best estimators to the current directory.

```classifier.py``` — the script which can be used to classify texts as described above. It will probably not spot a text generated by a LLM of today.
